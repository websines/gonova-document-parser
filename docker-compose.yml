version: '3.8'

# Full deployment: API server + vLLM backends
# Optimized for NVIDIA 3090 (24GB VRAM)

services:
  # Redis for job queue persistence
  redis:
    image: redis:7-alpine
    container_name: docparser-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - docparser
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5

  # Document Processing API Server
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: docparser-api
    ports:
      - "8080:8080"
    environment:
      # API Configuration
      - HOST=0.0.0.0
      - PORT=8080
      - WORKERS=1

      # Processing Configuration
      - DEFAULT_ACCURACY_MODE=balanced
      - INFERENCE_MODE=vllm
      - ENABLE_ENRICHMENT=true

      # vLLM Endpoints
      - VLLM_DEEPSEEK_URL=http://vllm-deepseek:8000/v1
      - VLLM_NANONETS_URL=http://vllm-nanonets:8001/v1
      - VLLM_GRANITE_URL=http://vllm-granite:8002/v1

      # Redis
      - REDIS_URL=redis://redis:6379/0

      # GPU Configuration (if running directly on host)
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_DEVICE=cpu  # API doesn't need GPU
    volumes:
      - ./uploads:/app/uploads
      - ./outputs:/app/outputs
      - ~/.cache/huggingface:/root/.cache/huggingface
    depends_on:
      redis:
        condition: service_healthy
      vllm-deepseek:
        condition: service_started
    networks:
      - docparser

  # DeepSeek-OCR vLLM Server (Primary processor)
  vllm-deepseek:
    image: vllm/vllm-openai:latest
    container_name: vllm-deepseek
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - --model=deepseek-ai/DeepSeek-OCR
      - --host=0.0.0.0
      - --port=8000
      - --dtype=bfloat16
      - --gpu-memory-utilization=0.85
      - --max-model-len=8192
      - --tensor-parallel-size=1
      - --trust-remote-code
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    shm_size: 8gb
    networks:
      - docparser
    profiles:
      - deepseek
      - full

  # Nanonets-OCR2-3B vLLM Server (Handwriting/Signatures)
  # Note: Start this separately when needed (not simultaneously with DeepSeek on 3090)
  vllm-nanonets:
    image: vllm/vllm-openai:latest
    container_name: vllm-nanonets
    ports:
      - "8001:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - --model=nanonets/Nanonets-OCR2-3B
      - --host=0.0.0.0
      - --port=8000
      - --dtype=bfloat16
      - --gpu-memory-utilization=0.85
      - --max-model-len=15000
      - --tensor-parallel-size=1
      - --trust-remote-code
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    shm_size: 8gb
    networks:
      - docparser
    profiles:
      - nanonets

  # Granite-Docling vLLM Server (Enrichment - smallest model)
  vllm-granite:
    image: vllm/vllm-openai:latest
    container_name: vllm-granite
    ports:
      - "8002:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command:
      - --model=ibm-granite/granite-docling-258m
      - --host=0.0.0.0
      - --port=8000
      - --dtype=bfloat16
      - --gpu-memory-utilization=0.5
      - --max-model-len=8192
      - --tensor-parallel-size=1
      - --trust-remote-code
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    shm_size: 4gb
    networks:
      - docparser
    profiles:
      - granite

networks:
  docparser:
    driver: bridge

volumes:
  redis-data:
    driver: local

# ==============================================================================
# Usage Instructions
# ==============================================================================
#
# For NVIDIA 3090 (24GB), run ONE vLLM service at a time:
#
# 1. Start API + DeepSeek (for standard documents):
#    docker compose --profile deepseek up -d
#
# 2. Stop DeepSeek and start Nanonets (for handwriting):
#    docker compose stop vllm-deepseek
#    docker compose --profile nanonets up -d vllm-nanonets
#
# 3. Check logs:
#    docker compose logs -f api
#    docker compose logs -f vllm-deepseek
#
# 4. Access API:
#    - API: http://localhost:8080
#    - Docs: http://localhost:8080/docs
#    - Health: http://localhost:8080/health
#
# 5. Stop all:
#    docker compose down
#
# ==============================================================================
