# Document Parser Configuration
# Copy this to .env and update with your settings

# GPU Configuration
CUDA_VISIBLE_DEVICES=0
TORCH_DEVICE=cuda
VRAM_LIMIT_GB=22  # Leave 2GB headroom on 3090 24GB

# Processing Mode
# Options: transformers, vllm
INFERENCE_MODE=transformers

# vLLM Configuration (if using vLLM hosting)
VLLM_DEEPSEEK_URL=http://localhost:8000/v1/chat/completions
VLLM_NANONETS_URL=http://localhost:8001/v1/chat/completions
VLLM_GRANITE_URL=http://localhost:8002/v1/chat/completions
VLLM_API_KEY=EMPTY

# Model Paths (auto-downloaded from HuggingFace if not specified)
DEEPSEEK_MODEL=deepseek-ai/DeepSeek-OCR
NANONETS_MODEL=nanonets/Nanonets-OCR2-3B
GRANITE_MODEL=ibm-granite/granite-docling-258m

# Processing Configuration
DEFAULT_ACCURACY_MODE=balanced  # fast, balanced, maximum
DEFAULT_BATCH_SIZE=4
MAX_PAGES_PER_BATCH=500
ENABLE_SIGNATURE_DETECTION=true
ENABLE_HANDWRITING_DETECTION=true

# Output Configuration
OUTPUT_DIR=./output
CACHE_DIR=./cache
LOG_LEVEL=INFO

# Graph-Vector DB (optional)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# API Server (optional)
API_HOST=0.0.0.0
API_PORT=8080
API_WORKERS=1

# Redis (for job queue persistence)
# Use redis://localhost:6379/0 for local deployment
# Use redis://redis:6379/0 for Docker Compose
REDIS_URL=redis://localhost:6379/0
