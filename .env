# Document Parser Configuration
# This application connects to EXTERNAL vLLM servers (does not deploy them)
# Uses PER-PAGE ROUTING for optimal speed and accuracy

# =============================================================================
# EXTERNAL vLLM SERVERS (Required)
# =============================================================================
# These should already be running on your host machine
# This application only connects to them via OpenAI-compatible API
#
# Per-Page Routing Strategy:
# - Each page classified individually using pypdf heuristics (< 0.05s per page)
# - DeepSeek: Fast processing for standard pages (60-70% of pages)
# - Nanonets: Specialized for legal docs, signatures, forms (30-40% of pages)
# - True parallel processing: 16 concurrent requests per model (matches vLLM --max-num-seqs)

# Processing Mode - USE vllm to connect to external servers
INFERENCE_MODE=vllm

# External vLLM Server URLs (OpenAI-compatible endpoints)
# Note: AsyncOpenAI automatically appends /chat/completions, so only provide the base URL
# Port 4444: DeepSeek-OCR (fast, standard pages)
VLLM_DEEPSEEK_URL=http://localhost:4444/v1
# Port 4445: Nanonets-OCR2-3B (legal, forms, signatures)
VLLM_NANONETS_URL=http://localhost:4445/v1
# Port 4446: Granite-Docling (optional enrichment layer)
VLLM_GRANITE_URL=http://localhost:4446/v1
VLLM_API_KEY=EMPTY

# Model Names (for reference - models run on your vLLM servers)
DEEPSEEK_MODEL=deepseek-ai/DeepSeek-OCR
NANONETS_MODEL=prithivMLmods/Nanonets-OCR2-3B-AWQ-nvfp4
GRANITE_MODEL=ibm-granite/granite-docling-258m

# =============================================================================
# GPU Configuration (NOT USED when INFERENCE_MODE=vllm)
# =============================================================================
# These settings only apply if you run models locally with INFERENCE_MODE=transformers
# When using external vLLM servers, this app doesn't need GPU access
CUDA_VISIBLE_DEVICES=0
TORCH_DEVICE=cpu  # API server doesn't need GPU when using vLLM
VRAM_LIMIT_GB=22

# Processing Configuration
DEFAULT_ACCURACY_MODE=balanced  # fast, balanced, maximum (used for metadata only, per-page routing ignores this)
DEFAULT_BATCH_SIZE=4  # DEPRECATED: async processor uses concurrency=16
MAX_PAGES_PER_BATCH=500  # DEPRECATED: async processor handles all pages
ENABLE_SIGNATURE_DETECTION=true  # Automatically routes signature pages to Nanonets
ENABLE_HANDWRITING_DETECTION=true  # Automatically routes handwriting to Nanonets

# Output Configuration
OUTPUT_DIR=./output
CACHE_DIR=./cache
LOG_LEVEL=INFO

# =============================================================================
# API Server Configuration
# =============================================================================
API_HOST=0.0.0.0
API_PORT=8080
API_WORKERS=1

# Background workers (for async job processing)
# These workers run automatically when you start the API
# Recommended: 1-2 workers per CPU core
NUM_WORKERS=2

# =============================================================================
# Redis Configuration (Required for async job processing)
# =============================================================================
# Must be running on your host machine at localhost:6379
REDIS_URL=redis://localhost:6379/0

# =============================================================================
# Optional Integrations
# =============================================================================
# Graph-Vector DB (for storing processed documents)
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
